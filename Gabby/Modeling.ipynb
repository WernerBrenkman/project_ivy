{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling \n",
    "Import .csv file in order for modelling to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_engineered.csv')\n",
    "data.head()\n",
    "\n",
    "# remove NaN comments from dataframe\n",
    "data = data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Split into Testing & Training\n",
    "\n",
    "Split data after all features 'required' have been added as additional columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & validation sets\n",
    "# For word embeddings, explicitly provide x = comments, target = label\n",
    "\n",
    "# only consider 'toxic' as the traget variable for now\n",
    "train_t = data.drop(['severe_toxic','obscene', 'threat', 'insult', 'identity_hate'], axis = 1)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_t['comment_text'], train_t['toxic'])\n",
    "\n",
    "# label encode the target variable (to feed into the algorithm)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>mentions</th>\n",
       "      <th>uppercase</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww match background colour im seemingly stuc...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man im really trying edit war guy constant...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>cant make real suggestion improvement wondered...</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  explanation edits made username hardcore metal...      0   \n",
       "1  000103f0d9cfb60f  daww match background colour im seemingly stuc...      0   \n",
       "2  000113f07ec002fd  hey man im really trying edit war guy constant...      0   \n",
       "3  0001b41b1c6bb37e  cant make real suggestion improvement wondered...      0   \n",
       "4  0001d958c54c6e35                sir hero chance remember page thats      0   \n",
       "\n",
       "   word_count  avg_word  mentions  uppercase  stop_words  sentiment  \n",
       "0          42         5         0          2          14   0.136364  \n",
       "1          18         5         0          1           1   0.200000  \n",
       "2          42         4         0          0          18   0.150000  \n",
       "3         112         4         0          5          49   0.183333  \n",
       "4          13         4         0          0           4   0.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "First focus is to wrod-embed our dataset. By doing so, we are able to convert out text to 'number' equivalents.\n",
    "(Map word to Vectors).\n",
    "\n",
    "This will be explored by Frequency X, broken down as follows:\n",
    "- Count Vector\n",
    "- TF-IDF Vector\n",
    "- (Co-Occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vector\n",
    "\n",
    "count_vector = CountVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}')\n",
    "count_vector.fit(data['comment_text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vector.transform(train_x)\n",
    "xvalid_count =  count_vector.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119664x237008 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 3316977 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer= 'word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(data['comment_text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119664x5000 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 2716659 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf\n",
    "\n",
    "# 2 elements doesn't seem right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In an effort to build a utility function that will train a model based on inputs.\n",
    "\n",
    "def train_model(classifier, train_set, target, valid_set):\n",
    "    # fit the training data on classifier\n",
    "    classifier.fit(train_set, target)\n",
    "    \n",
    "    # predict on validation set\n",
    "    predictions = classifier.predict(valid_set)\n",
    "    \n",
    "    # predict on test set & save results (csv)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Count Vector:  0.942388688327\n",
      "Naive Bayes - TF-IDF:  0.951940433213\n"
     ]
    }
   ],
   "source": [
    "# on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print \"Naive Bayes - Count Vector: \", accuracy\n",
    "\n",
    "# on TF-IDF vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print \"Naive Bayes - TF-IDF: \", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Count Vector:  0.95326915363\n",
      "Logistic Regression - TF-IDF:  0.958558965102\n"
     ]
    }
   ],
   "source": [
    "# on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print \"Logistic Regression - Count Vector: \", accuracy\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print \"Logistic Regression - TF-IDF: \", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vars = data.drop(['id','comment_text','toxic','severe_toxic','obscene', 'threat', 'insult', 'identity_hate'], axis = 1)\n",
    "target = data['toxic']\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data_vars, target, test_size=0.2)\n",
    "\n",
    "logit = linear_model.LogisticRegression()\n",
    "logit.fit(x_train, y_train)\n",
    "\n",
    "predictions = logit.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.907430039798\n"
     ]
    }
   ],
   "source": [
    "predictions\n",
    "\n",
    "accuracy = logit.score(x_test, y_test)\n",
    "print \"Logistic Regression: \", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0005300084f90edc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00054a5e18b50dd4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0006f16e4e9f292e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00070ef96486d6f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00078f8ce7eb276d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000897889268bc93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0009801bd85e5806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0009eaea3325de8c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>000b08c464718505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>000bfd0867774845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000c0dfd995809fa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>000c6a3f0cd3ba8e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>000cfee90f50d471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>000eefc67a2c930f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>000f35deef84dc4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000ffab30195c5e1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0010307a3a50a353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0010833a96e1f886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31881</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31882</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31883</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31884</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31885</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31886</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31887</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31888</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31889</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31890</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31891</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31892</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31893</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31894</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31895</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31896</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31897</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31898</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31899</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31900</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31901</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31902</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31903</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31904</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31905</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31906</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31907</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31908</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31909</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31910</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191463 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "0      0000997932d777bf\n",
       "1      000103f0d9cfb60f\n",
       "2      000113f07ec002fd\n",
       "3      0001b41b1c6bb37e\n",
       "4      0001d958c54c6e35\n",
       "5      00025465d4725e87\n",
       "6      0002bcb3da6cb337\n",
       "7      00031b1e95af7921\n",
       "8      00037261f536c51d\n",
       "9      00040093b2687caa\n",
       "10     0005300084f90edc\n",
       "11     00054a5e18b50dd4\n",
       "12     0005c987bdfc9d4b\n",
       "13     0006f16e4e9f292e\n",
       "14     00070ef96486d6f9\n",
       "15     00078f8ce7eb276d\n",
       "16     0007e25b2121310b\n",
       "17     000897889268bc93\n",
       "18     0009801bd85e5806\n",
       "19     0009eaea3325de8c\n",
       "20     000b08c464718505\n",
       "21     000bfd0867774845\n",
       "22     000c0dfd995809fa\n",
       "23     000c6a3f0cd3ba8e\n",
       "24     000cfee90f50d471\n",
       "25     000eefc67a2c930f\n",
       "26     000f35deef84dc4a\n",
       "27     000ffab30195c5e1\n",
       "28     0010307a3a50a353\n",
       "29     0010833a96e1f886\n",
       "...                 ...\n",
       "31881                 0\n",
       "31882                 0\n",
       "31883                 0\n",
       "31884                 0\n",
       "31885                 0\n",
       "31886                 0\n",
       "31887                 0\n",
       "31888                 0\n",
       "31889                 0\n",
       "31890                 0\n",
       "31891                 0\n",
       "31892                 0\n",
       "31893                 0\n",
       "31894                 0\n",
       "31895                 0\n",
       "31896                 0\n",
       "31897                 0\n",
       "31898                 0\n",
       "31899                 0\n",
       "31900                 0\n",
       "31901                 0\n",
       "31902                 0\n",
       "31903                 0\n",
       "31904                 0\n",
       "31905                 0\n",
       "31906                 0\n",
       "31907                 0\n",
       "31908                 0\n",
       "31909                 0\n",
       "31910                 0\n",
       "\n",
       "[191463 rows x 1 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Help here?\n",
    "\n",
    "pd.concat([data['id'], pd.DataFrame(predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
