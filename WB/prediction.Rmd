---
title: "Prediction"
output: github_document
---

```{r setup, include=FALSE, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source('app.R')

# remove redundant vars
redunant_vars = c('set_id', 'Id', 'SalePrice', 'SaleCondition', 'SaleType', 'MSSubClass')
train[, redunant_vars] = NULL
test[, redunant_vars[-2]] = NULL
df[, redunant_vars] = NULL

# functions
f_err = function(actual, predicted){
  return(sqrt(mean((actual - predicted)^2)))
}
f_print_results = function(){
  results = data.frame(
    AdjRsq = lapply(lm_models, '[[', 'arsq') %>% unlist() %>% round(., 3),
    TrainError = lapply(lm_models, '[[', 'err') %>% unlist() %>% round(., 3),
    TestError = lapply(lm_models, '[[', 'pred_score') %>% unlist() %>% round(., 3))
  print(results)
}
f_update_model = function(formula = NA, pred_score = NA, do_predict = TRUE, step_model = NULL){
  
  # formula
  # ------------
  
  if(is_null(step_model)){
    formula = formula
  }else{
    formula = formula(step_model)
  }
  # fit
  # ------------
  if(is_null(step_model)){
    fit = lm(formula = formula, data = train)
  }else{
    fit = step_model
  }
  
  err = f_err(train$SalePriceLog, fit$fitted.values)
  arsq = summary(fit)$adj.r.squared
  pred = NA
  if(do_predict){
    pred = data.frame(Id = test$Id, SalePrice = 10^predict(fit, test))
  }
  pred_score = pred_score
  return(list(formula = formula, fit = fit, err = err, arsq = arsq, pred = pred, pred_score = pred_score))
}
```


## Multiple linear regression

```{r lm_models, include=FALSE}
# formulae
s_area = paste(c(area_vars, 'area_cl'), collapse = ' + ') %>% paste('SalePriceLog ~ ', .)
f_area = as.formula(s_area)
f_area_cl = paste0(s_area, ' + area_cl') %>% as.formula()
qual_vars = c('OverallQual',
           'ExterQual',
           'BsmtQual',
           'HeatingQC',
           'LowQualFinSF',
           'KitchenQual',
           'FireplaceQu',
           'GarageQual',
           'PoolQC',
           'Fence'
           )

s_qual = qual_vars %>% paste(., collapse = ' + ') %>% paste('SalePriceLog ~ ', .)
f_qual = as.formula(s_qual)

# list to keep all models
lm_models = list(satured = list(), area = list(), cluster = list(), qual = list())

# saturated model
lm_models$satured = f_update_model(as.formula(SalePriceLog ~ .), 0.14184)

# area related model
lm_models$area = f_update_model(f_area, 0.16932)

# area cluster & pca related model
lm_models$cluster = f_update_model(as.formula(SalePriceLog ~ PC1 + area_cl), 0.16616)

# qual model
lm_models$qual = f_update_model(f_qual, 0.20303)

# export test results
write_csv(lm_models$satured$pred, 'data/pred_satured.csv')
write_csv(lm_models$area$pred, 'data/pred_area.csv')
write_csv(lm_models$cluster$pred, 'data/pred_cluster.csv')
write_csv(lm_models$qual$pred, 'data/pred_qual.csv')

```
At a first glance we tried 4 different multiple linear regression models. 

+ saturated: model containing al variables
+ area: model with only area related varialbes
+ cluster: model using the first 7 principal components (with addresses 90% of the total variance) and the area cluster
+ qual: model containing only quality related variables, which have been enumerated.

The results of these models are as follows:

```{r lm_results}
# results
f_print_results()
```

Let's use the area and cluster models as examples to see if we can further improve the accuracy.
```{r}
require(car)
vif(lm_models$area$fit)
summary(lm_models$area$fit)
```
```{r}
require(car)
vif(lm_models$cluster$fit)
summary(lm_models$cluster$fit)
```
Let's remove variables that where the VIF > 5 **or** are not significant. 
For the cluster model these include: **GrLivArea**, **LotFrontage**, **MasVnrArea**.
For the cluster model these include: **PC6**, **PC8**, **PC13**.

```{r}
lm_models$area_upd = f_update_model(update.formula(lm_models$area$formula, ~. - GrLivArea - LotFrontage - MasVnrArea))
lm_models$cluster_upd = f_update_model(update.formula(lm_models$cluster$formula, ~. - PC6 - PC8 - PC13))

anova(lm_models$area$fit, lm_models$area_upd$fit)
anova(lm_models$cluster$fit, lm_models$cluster_upd$fit)

results = data.frame(
  AdjRsq = lapply(lm_models, '[[', 'arsq') %>% unlist() %>% round(., 3),
  TrainError = lapply(lm_models, '[[', 'err') %>% unlist() %>% round(., 3),
  TestError = lapply(lm_models, '[[', 'pred_score') %>% unlist() %>% round(., 3))
f_print_results()
```
The anova indicates that we can remove these variables, without any loss of information, but we didn't gain anything in terms of accuracy either. 

Going through the above process for the saturated model is going to be extremely time consuming. Especially if it only means gaining efficiency and not accuracy. There must be a better way. One option would be to automate the variable selection via stepwise procedure.

```{r}
require(MASS)

lm_models$area_step = f_update_model(step_model = step(lm_models$area$fit,
                                          direction = "both",
                                          k=2,
                                          trace = FALSE,
                                          scope = list(lower = formula(lm(SalePriceLog ~ 1, data = train)),
                                                       upper = lm_models$area$formula)))

lm_models$cluster_step = f_update_model(step_model = step(lm_models$cluster$fit,
                                          direction = "both",
                                          k=2,
                                          trace = FALSE,
                                          scope = list(lower = formula(lm(SalePriceLog ~ 1, data = train)),
                                                       upper = lm_models$cluster$formula)))

f_print_results()
print(c(lm_models$area_upd$formula, lm_models$area_step$formula))
print(c(lm_models$cluster_upd$formula, lm_models$cluster_step$formula))


```
Notice that the variables chosen by the step procedure are the same as those chosen manually in the previous step, but again no improvement in accuracy. Let's now extend this idea to the **saturated** model.

```{r}


lm_models$saturated_step = f_update_model(do_predict = TRUE,
                                          step_model = step(lm_models$satured$fit,
                                              direction = "both",
                                              k=2,
                                              trace = FALSE,
                                              scope = list(lower = formula(lm(SalePriceLog ~ 1, data = train)),
                                                           upper = lm_models$satured$formula)))
write_csv(lm_models$saturated_step$pred, 'data/pred_satured_step.csv')
lm_models$saturated_step$pred_score = 0.13136
f_print_results()
```

What can we learn from the saturated_step model?
```{r}
summary(lm_models$saturated_step$fit)

```
This works and has produced the best result yet, but the alorithm is very slow. Let's have a look at other algorithms.
 
## Regularization

```{r}
require(glmnet)

# prep data
# ---------------
X = dplyr::select(train, -SalePriceLog) %>%
  data.matrix()
Y = dplyr::select(train, SalePriceLog) %>%
  data.matrix()
Z = dplyr::select(test, -Id) %>%
  data.matrix()
grid_r = 10^seq(2, -4, length = 200)
grid_l = 10^seq(2, -4, length = 200)
i = sample(1:nrow(X), 7*nrow(X)/10)


# fit
# ---------------
reg_models = list(ridge = list(), lasso = list())
reg_models$ridge$fit = glmnet(x = X[i,], y = Y[i], lambda = grid_r, alpha = 0)
reg_models$lasso$fit = glmnet(x = X[i,], y = Y[i], lambda = grid_l, alpha = 1)

plot(reg_models$ridge$fit, xvar = "lambda", label = TRUE, main = "Ridge Regression")
plot(reg_models$lasso$fit, xvar = "lambda", label = TRUE, main = "Ridge Regression")

# optim lambda
# ---------------

reg_models$ridge$cv = cv.glmnet(x = X, y = Y, lambda = grid_r, alpha = 0, nfolds = 10)
reg_models$lasso$cv = cv.glmnet(x = X, y = Y, lambda = grid_l, alpha = 1, nfolds = 10)

plot(reg_models$ridge$cv)
plot(reg_models$lasso$cv)

reg_models$ridge$lambda = reg_models$ridge$cv$lambda.min
reg_models$lasso$lambda = reg_models$lasso$cv$lambda.min

# error
# ---------------
reg_models$ridge$error = sqrt(mean((predict(reg_models$ridge$fit, s = reg_models$ridge$lambda, newx = X[-i, ]) - Y[-i])^2))
reg_models$lasso$error = sqrt(mean((predict(reg_models$lasso$fit, s = reg_models$lasso$lambda, newx = X[-i, ]) - Y[-i])^2))
data.frame(ErrorRate = lapply(reg_models, '[[', 'error') %>% unlist() %>% round(., 3))

# predict
# ---------------
reg_models$ridge$pred = data.frame(Id = test$Id, SalePrice = 10^predict(reg_models$ridge$fit, s = reg_models$ridge$lambda, newx = Z) %>% as.vector())
reg_models$lasso$pred = data.frame(Id = test$Id, SalePrice = 10^predict(reg_models$lasso$fit, s = reg_models$lasso$lambda, newx = Z) %>% as.vector())

write_csv(reg_models$ridge$pred, 'data/pred_ridge.csv')
write_csv(reg_models$lasso$pred, 'data/pred_lasso.csv')

reg_models$ridge$pred_error = 0.13911
reg_models$lasso$pred_error = 0.13798
```
But what can we learn? Which variables are redundant?

```{r}

reg_models$ridge$lambda
coef(reg_models$ridge$fit)[, which(reg_models$ridge$fit$lambda == reg_models$ridge$lambda )]


reg_models$lasso$lambda
c = coef(reg_models$lasso$fit)[, min(which(reg_models$lasso$fit$lambda <= reg_models$lasso$lambda ))]
c[c != 0] %>% names() %>% sort()

```

## Random forest

```{r}
require(randomForest)

optim = FALSE
if(optim){
  grid = seq(30,50,2)
  oob.err = numeric(length(grid))
  for (idx in 1:length(grid)) {
    mtry = grid[idx]
    fit = randomForest(SalePriceLog ~ ., data = train, mtry = mtry)
    oob.err[idx] = fit$mse[500]
    cat("We're performing iteration", mtry, "\n")
  }
  
  #Visualizing the OOB error.
  plot(grid, oob.err, pch = 16, type = "b",
       xlab = "Variables Considered at Each Split",
       ylab = "OOB Mean Squared Error",
       main = "Random Forest OOB Error Rates\nby # of Variables")
  
  best_mtry = grid[which.min(oob.err)]
}

#Fitting an initial random forest to the training subset.
set.seed(0)
rf_fit = randomForest(SalePriceLog ~ ., data = train, importance = TRUE)

varImp = importance(rf_fit) %>% as.data.frame()
varImp[order(-varImp$`%IncMSE`), ]
varImpPlot(rf_fit)

rf_pred = 10^predict(rf_fit, test)
write_csv(data.frame(Id = test$Id, SalePrice = rf_pred %>% as.vector()), 'data/pred_rf.csv')
# 0.14080
# 
```


