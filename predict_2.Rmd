---
title: "Pred"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source('app.R')


# remove redundant vars
redunant_vars = c('set_id', 'Id', 'SalePrice', 'SaleCondition', 'SaleType', 'MSSubClass', 'Neighborhood')
train[, redunant_vars] = NULL
test[, redunant_vars[-2]] = NULL
df[, redunant_vars] = NULL


# functions
f_err = function(actual, predicted){
  return(sqrt(mean((actual - predicted)^2)))
}
f_print_results = function(){
  results = data.frame(
    AdjRsq = lapply(lm_models, '[[', 'arsq') %>% unlist() %>% round(., 3),
    TrainError = lapply(lm_models, '[[', 'err') %>% unlist() %>% round(., 3),
    TestError = lapply(lm_models, '[[', 'pred_score') %>% unlist() %>% round(., 3))
  print(results)
}
```

# Multiple linear regression

*  Fit saturated model to get a benchmark
*  Fit intuitive model
*  Variable selection using step procedure
*  Diagnostics
*  Predict and submit to Kaggle

#### Saturated model

To start we are going to fit a model with all available variables. This model will serve as the benchmark.

```{r saturated, warning=FALSE, message=FALSE}
# models list object
mdls = list(saturated = list(), intuition = list(), step = list())

# fit saturated model
mdls$saturated$formula = formula(SalePriceLog ~ .)
mdls$saturated$fit = lm(mdls$saturated$formula, data = train)
mdls$saturated$train_error = sqrt(mean(mdls$saturated$fit$residuals^2))

# predictions
mdls$saturated$pred = data.frame(Id = test$Id, SalePrice = 10^predict(mdls$saturated$fit, test))


# diagnostic plots 
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
plot(mdls$saturated$fit)
anova(mdls$saturated$fit)

```
There are too many variables in this model to plot here instead we plot the results of the anova test. We have a look at the diagnostic plots, anova results and note that the Adj. R-Squared value for this model is **`r summary(mdls$saturated$fit)$adj.r.squared`**.

#### Intuitive model

The **intuitive** model is built on variables that seems to make intuitive sense.

```{r intuition, warning=FALSE, message=FALSE}
# fit intuition model
mdls$intuition$formula = formula(SalePriceLog ~ TotalBsmtSF + NeighPrice + OverallQual + OverallCond + YearBuilt + GrLivArea + BsmtQual + KitchenQual + HeatingQC + FireplaceQu + GarageArea + CentralAir + LotArea + LotShape + BsmtFinSF1 + x_1stFlrSF)
mdls$intuition$fit = lm(mdls$intuition$formula, data = train)
mdls$intuition$train_error = sqrt(mean(mdls$intuition$fit$residuals^2))

# predictions
mdls$intuition$pred = data.frame(Id = test$Id, SalePrice = 10^predict(mdls$intuition$fit, test))

# diagnostic plots 
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
plot(mdls$intuition$fit)
anova(mdls$intuition$fit)

```
We have a look at the diagnostic plots and note that the Adj. R-Squared value for this model is **`r summary(mdls$intuition$fit)$adj.r.squared`**.

#### Variable selection using stepwise procedure

Using a saturated model isn't practical and adds additional noise. It could also lead to over-fitting and poor generalization. We will try to reduce the number variables to only significant variables in a automated fashion. Let's choose variables by AIC in a Stepwise Algorithm. 

```{r step, warning=FALSE, message=FALSE}

# fit step model
require(MASS)
set.seed(2)
mdls$step$fit = step(mdls$saturated$fit,
                     direction = "both",
                     k=2,
                     trace = FALSE,
                     scope = list(lower = formula(lm(SalePriceLog ~ 1, data = train)),
                                  upper = mdls$saturated$formula))
mdls$step$formula = formula(mdls$step$fit)
mdls$step$train_error = sqrt(mean(mdls$step$fit$residuals^2))

# predictions
mdls$step$pred = data.frame(Id = test$Id, SalePrice = 10^predict(mdls$step$fit, test))

# diagnostic plots 
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
plot(mdls$step$fit)
anova(mdls$step$fit)

```

The residual plot doesn't display any patterns like fanning, but the QQ-plot does indicate an element of non-normality. This indicates that the errors are not random and that the model does **not** fully explain all the variance.

The stepwise model suggest using `r nrow(anova(mdls$step$fit))` of the original `r nrow(anova(mdls$saturated$fit))` variables. That is a reduction of `r nrow(anova(mdls$saturated$fit)) - nrow(anova(mdls$step$fit))`, but are we losing any information? Use anova test to assess the impact.

```{r compare1, warning=FALSE, message=FALSE} 
# compare step vs saturated
anova(mdls$step$fit, mdls$saturated$fit)
```

The anova test indicates that we can remove these variables, without significant loss of information.
The **step** model attained and Adj. R-Squared value of `r summary(mdls$step$fit)$adj.r.squared`, which is very high. It is also very similar to that of the **saturated** model. The summary indicates that there are multiple variables that are significant. What is worrying is that many of the levels in the various factor variables are not significant. Maybe additional feature engineering could be done to isolate significant levels. It is out of scope for this project.

```{r compare1_2, warning=FALSE, message=FALSE} 
# compare step vs intuition
anova(mdls$step$fit, mdls$intuition$fit)
```

It is interesting to note that there is a significant difference between the **step** & **intuition** models, but both still have an Adj. R-Squared value of greater than 0.8.

```{r diag3, warning=FALSE, message=FALSE}
# diagnostics
require(car)
set.seed(0)
vif(mdls$step$fit)
# sort(v[v > 5])
```

VIF indicates that **PC1** and **rat_1stFlr_GrLiv** should be removed, but the values are border-line. Let's assess the impact of removing these variables.

```{r compare2, warning=FALSE, message=FALSE}
# compare step vs step_reduced
mdls$step_reduced$formula = update.formula(mdls$step$formula, ~ . - x_1stFlrSF - rat_1stFlr_GrLiv)
mdls$step_reduced$fit = lm(mdls$step_reduced$formula, data = train)
mdls$step_reduced$train_error = sqrt(mean(mdls$step_reduced$fit$residuals^2))
anova(mdls$step_reduced$fit, mdls$step$fit)


mdls$step_reduced$pred = data.frame(Id = test$Id, SalePrice = 10^predict(mdls$step_reduced$fit, test))
write_csv(mdls$step_reduced$pred, 'data/step_reduced.csv')

```
The anova test suggest that removal of **PC1** and **rat_1stFlr_GrLiv** leads to a significant different output, hence we should not remove these variables.

#### Prediction

We now predict using the **test** data and submit it to Kaggle for validation. One can see that the **step** model achieved slightly better results than the **saturated** model although we have removed `r nrow(anova(mdls$saturated$fit)) - nrow(anova(mdls$step$fit))` variables.
```{r predict1, warning=FALSE, message=FALSE}
write_csv(mdls$saturated$pred, 'data/saturated.csv')
write_csv(mdls$intuition$pred, 'data/intuition.csv')
write_csv(mdls$step$pred, 'data/step.csv')
mdls$saturated$test_error = 0.13799
mdls$step_reduced$test_error = 0.13766
mdls$intuition$test_error = 0.13956
mdls$step$test_error = 0.13308

results = data.frame(
  TrainError = lapply(mdls, '[[', 'train_error') %>% unlist() %>% round(., 3),
  TestError = lapply(mdls, '[[', 'test_error') %>% unlist() %>% round(., 3))
print (results)
```

# Regularization

Next we will investigate **ridge** & **lasso** regression models. These algorithms are known for efficiency and accuracy.

```{r regularization, warning=FALSE, message=FALSE}
require(glmnet)

# prep data
# ---------------
X = dplyr::select(train, -SalePriceLog) %>%
  data.matrix()
Y = dplyr::select(train, SalePriceLog) %>%
  data.matrix()
Z = dplyr::select(test, -Id) %>%
  data.matrix()
grid_r = 10^seq(5, -5, length = 1000)
grid_l = 10^seq(5, -5, length = 1000)
# i = sample(1:nrow(X), 7*nrow(X)/10)


# fit
# ---------------
set.seed(2)
mdls$ridge$fit = glmnet(x = X, y = Y, lambda = grid_r, alpha = 0)
set.seed(2)
mdls$lasso$fit = glmnet(x = X, y = Y, lambda = grid_l, alpha = 1)


# optim lambda using cross-validation
# ---------------
set.seed(2)
mdls$ridge$cv = cv.glmnet(x = X, y = Y, lambda = grid_r, alpha = 0, nfolds = 10)
set.seed(2)
mdls$lasso$cv = cv.glmnet(x = X, y = Y, lambda = grid_l, alpha = 1, nfolds = 10)

mdls$ridge$lambda = mdls$ridge$cv$lambda.min
mdls$lasso$lambda = mdls$lasso$cv$lambda.min

# error
# ---------------
mdls$ridge$train_error = sqrt(mean((predict(mdls$ridge$fit, s = mdls$ridge$lambda, newx = X) - Y)^2))
mdls$lasso$train_error = sqrt(mean((predict(mdls$lasso$fit, s = mdls$lasso$lambda, newx = X) - Y)^2))

# predict
# ---------------
mdls$ridge$pred = data.frame(Id = test$Id, SalePrice = 10^predict(mdls$ridge$fit, s = mdls$ridge$lambda, newx = Z) %>% as.vector())
mdls$lasso$pred = data.frame(Id = test$Id, SalePrice = 10^predict(mdls$lasso$fit, s = mdls$lasso$lambda, newx = Z) %>% as.vector())

# plot
# ---------------
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
plot(mdls$ridge$fit, xvar = "lambda", label = TRUE, main = "Ridge Regression")
plot(mdls$lasso$fit, xvar = "lambda", label = TRUE, main = "Lasso Regression")
plot(mdls$ridge$cv)
plot(mdls$lasso$cv)

```

#### Prediction

We now predict using the **test** data and submit it to Kaggle for validation. The test results for the **lasso** is comparable to that of the **step** model with the added benefit of simplicity of use and processing time. 

```{r predict2, warning=FALSE, message=FALSE}
write_csv(mdls$ridge$pred, 'data/ridge.csv')
write_csv(mdls$lasso$pred, 'data/lasso.csv')
mdls$ridge$test_error = 0.13645
mdls$lasso$test_error = 0.13399

results = data.frame(
  TrainError = lapply(mdls, '[[', 'train_error') %>% unlist() %>% round(., 3),
  TestError = lapply(mdls, '[[', 'test_error') %>% unlist() %>% round(., 3))
print (results)
```

It would be interesting to compare the significant variables identified by both models.
```{r compare4, , warning=FALSE, message=FALSE}
# get lasso variables
lasso_vars = coef(mdls$lasso$fit)[, min(which(mdls$lasso$fit$lambda <= mdls$lasso$lambda ))]
lasso_vars = lasso_vars[lasso_vars != 0] %>% names()

# get step variables
step_vars = mdls$step$fit$coefficients %>% names()

# list variables that exists in both models
common_vars = intersect(lasso_vars, step_vars)
common_vars = common_vars[common_vars != "(Intercept)"]

require(ggplot2)
require(GGally)
ggcorr(train[, c(common_vars, 'SalePriceLog')], nbreaks = 4, palette = "RdGy", label = TRUE, label_size = 3, label_color = "white")

```


# RandomForest

The process:

*  Find the best value for **mtry**
*  Fit the model using best **mtry**
*  Check variable importance
*  Refit using only the best variable and assess performance
*  Predict and submit to Kaggle

```{r randomforest, warning=FALSE, message=FALSE}
require(randomForest)

optim = TRUE
if(optim){
  grid = seq(20,50,5)
  oob.err = numeric(length(grid))
  for (idx in 1:length(grid)) {
    mtry = grid[idx]
    fit = randomForest(SalePriceLog ~ ., data = train, mtry = mtry)
    oob.err[idx] = fit$mse[500]
    cat("We're performing iteration", mtry, "\n")
  }
  
  #Visualizing the OOB error.
  plot(grid, oob.err, pch = 16, type = "b",
       xlab = "Variables Considered at Each Split",
       ylab = "OOB Mean Squared Error",
       main = "Random Forest OOB Error Rates\nby # of Variables")
  
  best_mtry = grid[which.min(oob.err)]
} else {
  best_mtry = 40
}
```

From the plot one can see that the optimal value of **mtry** is `r best_mtry`.

```{r rf_fit, warning=FALSE, message=FALSE}
#Fitting an  random forest model to the train data using the best mtry
set.seed(0)
mdls$rf$best_mtry = best_mtry  
mdls$rf$formula = formula(SalePriceLog ~ .)
mdls$rf$fit = randomForest(mdls$rf$formula, data = train, importance = TRUE, mtry = mdls$rf$best_mtry)
mdls$rf$train_error = sqrt(mean((train$SalePriceLog - predict(mdls$rf$fit, train))^2))

# Predict
mdls$rf$pred = data.frame(Id = test$Id, SalePrice = 10^predict(mdls$rf$fit, test))
```


```{r importance, warning=FALSE, message=FALSE}
varImp = importance(mdls$rf$fit) %>% as.data.frame()
varImpPlot(mdls$rf$fit)
```

Use only top n variables where %IncMSE > 10 and fit a reduced randomForest model.

```{r}
rf_vars = rownames(varImp[varImp$`%IncMSE` > 10,])
mdls$rf_reduced$formula = paste('SalePriceLog ~ ', paste(rf_vars, collapse = ' + ')) %>% as.formula(.)
mdls$rf_reduced$fit = randomForest(mdls$rf_reduced$formula, data = train, importance = TRUE)
mdls$rf_reduced$train_error = sqrt(mean((train$SalePriceLog - predict(mdls$rf_reduced$fit, train))^2))

# Predict
mdls$rf_reduced$pred = data.frame(Id = test$Id, SalePrice = 10^predict(mdls$rf_reduced$fit, test))

```

### Prediction

We now predict using the **test** data and submit it to Kaggle for validation. The test results for both random forest models are worse than the regression models, but note that the training error is much better. This speaks to overfitting of the random forest models which leads to poor generalization.

```{r predict3, warning=FALSE, message=FALSE}
write_csv(mdls$rf$pred, 'data/rf.csv')
write_csv(mdls$rf_reduced$pred, 'data/rf_reduced.csv')
mdls$rf$test_error = 0.13729
mdls$rf_reduced$test_error = 0.13489

results = data.frame(
  TrainError = lapply(mdls, '[[', 'train_error') %>% unlist() %>% round(., 3),
  TestError = lapply(mdls, '[[', 'test_error') %>% unlist() %>% round(., 3))
print (results)
```


# Ensemble

*An ensemble is a supervised learning technique for combining multiple weak learners/models to produce a strong learner.* Next we combine the results of the previous models and take the mean predicted value for each observation. This makes use of the law of large numbers that basically says that the consensus of the group is more accurate than the individual.

```{r ensebmle, warning=FALSE, message=FALSE}

mdls$ensemble$pred = data.frame(Id = test$Id, 
                                SalePrice = .2*mdls$step$pred$SalePrice + 
                                  .2*mdls$ridge$pred$SalePrice + 
                                  .2*mdls$lasso$pred$SalePrice + 
                                  .2*mdls$rf$pred$SalePrice + 
                                  .2*mdls$rf_reduced$pred$SalePrice
                                  )
  
write_csv(mdls$ensemble$pred, 'data/ensemble.csv')
mdls$ensemble$train_error = NA
mdls$ensemble$test_error = 0.12888

results = data.frame(
  TrainError = lapply(mdls, '[[', 'train_error') %>% unlist() %>% round(., 3),
  TestError = lapply(mdls, '[[', 'test_error') %>% unlist() %>% round(., 3))
print (results)
```

