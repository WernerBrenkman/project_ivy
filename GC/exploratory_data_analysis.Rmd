---
title: "project ivy: EDA - Gabriella"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(knitr)
source('src/ini.R')
source('src/load.R')
source('src/transform.R')
colSums(is.na(df))
```

## Overview

[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)

## Let's get a feel for the data

Let's load the data and implement the rules based on **data_description.txt** and inspect the data

```{r overview, echo=FALSE, warning=FALSE}
# dimensions
# -----------
dims = dim(df)
names(dims) = c('Rows', 'Cols')
print(dims)

# NA's
# -----------
print_na_cols = function(){
  na_cols = lapply(df, function(x){sum(is.na(x))}) %>%
  unlist() %>%
  as.data.frame() # %>%
  rownames_to_column()
names(na_cols) = c('Variable', 'Missing')
na_cols %>%
  filter(Missing > 0) %>%
  print()
}
# print_na_cols()
```

There are 21 variables containing NA's. Note the following:

*  **SalesPrice** is missing because the **test** data was merged with the **train** data. That leaves us with 20 variables containing NA's.
*  **GarageYrBlt** will be NA if there is no garage on the property

Let's visualise missing
```{r impute_viz, echo=FALSE, warning=FALSE, message=FALSE}
require(VIM)
df %>%
  select(-GarageYrBlt, -SalePrice, -Id, -set_id) %>%
  aggr(.)

```

## HAven't run this code (don't think it should be split just yet?)
```{r imputed, echo=FALSE, warning=FALSE}
source('src/impute.R')
# print_na_cols()
colSums(is.na(df))
```

## Visualisation

Notice **SalePrice** vs **SalePriceLog**. We'll be using **SalePriceLog** from now on.

```{r viz, echo=FALSE, fig.width=10, fig.asp=.25}

p = ggplot(train) + theme_bw()
multiplot(p + geom_histogram(aes(x = SalePrice)),
          p + geom_histogram(aes(x = SalePriceLog)),
          cols = 2)

```

### Numeric variables
Let's get an overview of the numeric variables
```{r, fig.width=14, fig.asp=1, echo=FALSE, warning=FALSE, message=FALSE}
draw_corplot = function(var, scale_x_log10 = FALSE){
  if(scale_x_log10){  
    p + geom_point(aes_string(x = var, y = 'SalePriceLog'), alpha = .5) + ggtitle(var) + scale_x_log10()
  }else{  
    p + geom_point(aes_string(x = var, y = 'SalePriceLog'), alpha = .5) + ggtitle(var)
  }
}
plotlist = lapply(train, function(x){ifelse(class(x) %in% c('integer', 'numeric'), 1, NA)}) %>% unlist()
plotlist = names(plotlist)[which(plotlist == 1)]
plotlist = plotlist[plotlist != 'Id']
plotlist = lapply(plotlist, function(x){draw_corplot(x)})
multiplot(cols = 5, plotlist = plotlist)
```

When reading **data_descr.txt** it is clear that some variables are highly correlated purely by definition. Let's explore those in detail and decide which to keep.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
cor_def_vars = c('YearBuilt', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'YearRemodAdd')

cor(train[, cor_def_vars], use = "complete.obs")

multiplot(cols = 2, plotlist = list(
  p + geom_point(aes_string(x = 'YearBuilt', y = 'GarageYrBlt')),
  p + geom_point(aes_string(x = 'GarageCars', y = 'GarageArea'))
))

```
From the above we can see that there is a very high correlation between **YearBuilt** & **GarageYrBlt** as well as **GarageCars** & **GarageArea**. Another point to note is that each of **GarageYrBlt**, **GarageCars** & **GarageArea** indicates the lack of a garage on the property. Further we will use only **YearBuilt** and **GarageArea** as these variables contains info regarding the age of the property (and garage), if there is in fact a garage and its size.

It would make sense that bigger houses are more expensive. Let's cluster all area related variables and assess the impact of area on **SalePrice**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
area_vars = c('LotArea', 
              # 'MasVnrArea', 
              # 'BsmtUnfSF', 
              'TotalBsmtSF', 
              'GrLivArea',
              'GarageArea',
              # 'SalePriceLog',
              'OverallQual'
              # 'WoodDeckSF', 
              # 'OpenPorchSF', 
              # 'EnclosedPorch', 
              # 'x_3SsnPorch', 
              # 'ScreenPorch', 
              # 'PoolArea'
              )
df_area = scale(train[, area_vars], center = F)

K = lapply(1:20, function(k){kmeans(x = df_area, centers = k, nstart = 100)$tot.withinss}) %>%
  unlist()
plot(K)

km_area = kmeans(x = df_area, centers = 4, nstart = 100)
train$cl_area = km_area$cluster
cor(train[, c('cl_area', 'SalePriceLog')])

p = ggplot(train) + theme_bw()
p + geom_point(aes(x = ExterQual, y = SalePriceLog, color = as.factor(cl_area)), alpha=.5)
p + geom_point(aes(x = LandSlope, y = SalePriceLog, color = as.factor(cl_area)), alpha=.5)

require(ggfortify)
autoplot(km_area, data = df_area)

```


### Categorical variables
The transformation of the categorical varibles are enclosed in the transform.R source code. A summary of the transformation is here. 

If we consider the Utilities variable, we see that all house except one have all public utilities. Thus, this variable doesn't offer much enrichment for Preditcion & therefore removed.
(Removed in feature_eng.R code).
```{r}
table(df$Utilities)
```

Placed **src/feature_eng.R** here as to include varables that are dropped from the categorical analysis. 
```{r remove_vars, echo=FALSE, warning=FALSE, message=FALSE}
source('src/feature_eng.R')
```


## Visualization of Categorical Varibles (Boxplot)
```{r, fig.width= 14, fig.asp=1, echo=FALSE}
SP_mean = mean(train$SalePriceLog)
draw_boxplot = function(var){
  p + geom_boxplot(aes_string(x = var, y = 'SalePriceLog')) + ggtitle(var) +
    geom_hline(yintercept=SP_mean, linetype="dashed", color = "red") 
}
plotlist = lapply(train, function(x){ifelse(class(x) %in% c('factor'), 1, NA)}) %>% unlist()
plotlist = names(plotlist)[which(plotlist == 1)]
plotlist = lapply(plotlist, function(x){draw_boxplot(x)})
multiplot(cols = 5, plotlist = plotlist)
# bp_vars = c(factor_vars, ord_factor_vars)
# plotlist = lapply(bp_vars, function(x){draw_boxplot(x)})
```
When reading **data_descr.txt** it is clear that some variables are highly correlated purely by definition. Let's explore those in detail and decide which to keep.

Highly correlated by definition, results in the following 'grouped' variables:
- Property Variables: Street, Alley, LotShape, LandContour, LandSlope, LotConfig
- Dwelling/ Living Variables: BldgType, HouseStyle, MSSubClass, MSZoning, Neighborhood
- Utility: Utilities, Electrical
- Overall Quality & Condition: OverallQual, OverallCond, Functional
- Material of House: OverallQual, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, ExterQual, ExterCond, Foundation
- Roof: RoofStyle, RoofMatl
- Basement: BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2
- Heating: Heating, HeatingQC, CentralAir, FireplaceQu
- Garage: GarageType, GarageFinish, GarageQual, GarageCond, PavedDrive
- Misc: PoolQC, Fence, MiscFeature, KitchenQual
- Nature of Sale: SaleType, SaleCondition

## Label Encoding Categorical Variables
Let's ensure that all categorical features that are ordered are label encoded to a numeric factor. We will then use this in our correlation matrix to determine it's relationship to SalePriceLog. 
This label encoding has been included in the **src/transform.R** code. 
To note, features were encoded with 0 = none/ the worst in ascending order until the best value. These orders were identified in **data_descr.txt** file (by definition). 

```{r}
# Categorical varibles to consider for Correlation
library(corrplot)
colSums(is.na(df))

# Need to include SalePriceLog
train_cat = train[, c(spec_ord_fact, qual_fact, 'SalePriceLog')]
dim(train_cat)

# Need to change to numeric (train_cat)
cor_cat = cor(train_cat, use="pairwise.complete.obs", method = 'pearson')
# corrplot(cor_cat, method = 'circle')
corrplot(cor_cat, method = 'circle', order="hclust")
```
From the above correlations, we can see a couple of clusters that have formed from the soring method (hclust).
ExterQual seems to be highly correlated to BsmtQual, KitchenQual, HeatingQC & GarageFinish. I guess this makes sense intuitively. If a house is modern outside, it's usually a reflection of the quality & maintenance on the inside. 
We can also see that there's a relationship between the GarageCond and GarageQual & PavedDrive. 

Additionally, we can see that the features that contribute to SalePrice significantly are:
ExterQual, KitchenQual, GarageFinish, BsmtQual, FireplaceQu, HeatingQC. 
Additional factors to consider are:
BsmtExposure, BsmtFinType1, CentralAir, PavedDrive, BsmtCOnd, GarageQual, GarageCond

Let's visualize these relationships:
```{r}
C1 = ggplot(train, aes(x = train$ExterQual, y = train$SalePriceLog)) + geom_point() + 
      geom_smooth(method=lm , color="red", se=FALSE)
C2 = ggplot(train, aes(x = train$KitchenQual, y = train$SalePriceLog)) + geom_point() + 
      geom_smooth(method=lm , color="red", se=FALSE)
C3 = ggplot(train, aes(x = train$GarageFinish, y = train$SalePriceLog)) + geom_point() + 
      geom_smooth(method=lm , color="red", se=FALSE)
C4 = ggplot(train, aes(x = train$BsmtQual, y = train$SalePriceLog)) + geom_point() + 
      geom_smooth(method=lm , color="red", se=FALSE)
C5 = ggplot(train, aes(x = train$FireplaceQu, y = train$SalePriceLog)) + geom_point() + 
      geom_smooth(method=lm , color="red", se=FALSE)
C6 = ggplot(train, aes(x = train$HeatingQC., y = train$SalePriceLog)) + geom_point() + 
      geom_smooth(method=lm , color="red", se=FALSE)
C3
library(gridExtra)
grid.arrange(C1, C2, C3, C4, C5, C6)
```
From the above graphs, we can see a very clear and strong postive linear relationship between the feature and SalesPrice. 

## Considering non-ordinal categorical features
From the list of of non-ordinal features, there are a couple that stand out and are worth exploring.
(It's more art than science).

# Neighborhood
One of the features I'm particularly interested in is the Neighborhood. From intuition, I know that neighborhood plays a pretty important role when someone is considering buying a house. More favourable Neighborhoods are likely to have higher SalePrices. Conversely, more popular neighborhoods could also mean cheaper SalesPrices. 

```{r}
N1 = ggplot(train, aes(x = Neighborhood)) + geom_histogram(stat = 'count') +
       theme(axis.text.x = element_text(angle = 90, hjust =1))
N1
```
From the above graph, we can see that NAmes and CollgCr are the most 'popular' neighborhoods (higher count). NAmes seems to have some variance that can't be ignored. Popularity could be dependant on many things, firstly it could be the price. Ie. Houses which are more affordable are more popular, additionally it could be based on the actual area of the neighborhood. 

Let's sort the neighborhood by highest median price. Reason to use median is to be less influenced by outliers of which there appear to be many in Neighborhood. In doing so, we hope to uncover some binning opportunities due to the fact that there are so many levels in Neighborhood. 

```{r}
SP_m = median(train$SalePrice)

N2 = ggplot(train, aes(x=reorder(Neighborhood, SalePrice, FUN = median), y = SalePrice)) +
      geom_bar(stat = 'summary', fun.y = 'median') +
      labs(x = 'Neighborhood', y = 'Median Sales Price') + 
      geom_hline(yintercept=SP_m, linetype="dashed", color = "red")  + 
      theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
      scale_y_continuous(breaks= seq(0, 800000, by=50000)) +
      geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=2)

N2
```
From the above graph, we can see that we could explore 3 potential ordinal levels of SalePrices (low, average & high). It's easy to identify the 'high' salesprices (indicative of a wealthy neighborhood) because there is a big difference between the top 3 and the rest of the neighborhoods. 
Ordinal Levels:
2 = Wealthy (higher priced)
1 = Average Priced
0 = Low Priced

```{r}
df$NeighPrice[df$Neighborhood %in% c('NridgHt', 'NoRidge', 'StoneBr')] = 2
df$NeighPrice[!df$Neighborhood %in% c('MeadowV', 'IDOTRR', 'BrDale', 'OldTown', 'Edwards', 'BrkSide', 'NridgHt', 'NoRidge', 'StoneBr')] = 1
df$NeighPrice[df$Neighborhood %in% c('MeadowV', 'IDOTRR', 'BrDale', 'OldTown', 'Edwards', 'BrkSide')] = 0

table(df$NeighPrice)
# Fix here
```

#MSSubClass
```{r}
M1 = ggplot(train, aes(x = MSSubClass)) + geom_histogram(stat = 'count') +
       theme(axis.text.x = element_text(angle = 90, hjust =1))
M1
```
Comparing to Mean Price
```{r}
SP_mean = mean(train$SalePrice)

N2 = ggplot(train, aes(x=reorder(MSSubClass, SalePrice, FUN = mean), y = SalePrice)) +
      geom_bar(stat = 'summary', fun.y = 'mean') +
      labs(x = 'MSSubClass', y = 'Mean Sales Price') + 
      geom_hline(yintercept=SP_mean, linetype="dashed", color = "red")  + 
      theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
      scale_y_continuous(breaks= seq(0, 800000, by=50000)) +
      geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=2)

N2
```
Top MSSubClasses = 
2-STORY 1946 & NEWER
1-STORY PUD (Planned Unit Development) - 1946 & NEWER
2-1/2 STORY ALL AGES
1-STORY 1946 & NEWER ALL STYLES

Average MSSubClasses = 
SPLIT OR MULTI-LEVEL
2-STORY 1945 & OLDER
1-STORY W/FINISHED ATTIC ALL AGES
SPLIT FOYER
1-1/2 STORY FINISHED ALL AGES
2-STORY PUD - 1946 & NEWER
DUPLEX - ALL STYLES AND AGES
2 FAMILY CONVERSION - ALL STYLES AND AGES

Bottom MSSubClasses = 
1-1/2 STORY - UNFINISHED ALL AGES
PUD - MULTILEVEL - INCL SPLIT LEV/FOYER
1-STORY 1945 & OLDER

```{r}
df$MSSubClassPrice[df$MSSubClass %in% c('60', '120', '75', '20')] = 2
df$MSSubClassPrice[!df$MSSubClass %in% c('30', '180', '45', '60','120', '75', '20' )] = 1
df$MSSubClassPrice[df$MSSubClass %in% c('30', '180', '45')] = 0

table(df$MSSubClassPrice)
```


# Feature Importance: Random Forest
```{r}
library(randomForest)

# sapply(train_rand, function(x) sum(is.na(x)))

# 
# rand_x = subset(train, select = -GarageYrBlt)
# rand_x = subset(rand_x, select = -SalePrice)
# rand_x = subset(rand_x, select = -SalePriceLog)
# rand_x = rand_x %>% mutate_if(is.character, as.factor)
# 
# rand_y = as.factor(train$SalePriceLog)
# 
# train_rand = subset(train, select = -GarageYrBlt)
# 
# quickRF = randomForest(x = rand_x, y = rand_y, ntree = 100, importance =TRUE)
# imp_RF = importance(quickRF)
# imp_DF = data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
# imp_DF = imp_DF[order(imp_DF$MSE, decreasing = TRUE),]

# sapply(train, function(x) sum(is.na(x)))
# train = subset(train, select = -GarageYrBlt)

train=train %>% mutate_if(is.character, as.factor)

fit = randomForest(SalePriceLog ~ . -SalePrice,
                   data = train, 
                   importance = TRUE,
                   tree = 100
                   )

varImpPlot(fit, n.var = min(20, nrow(fit$importance)))

```

To explore these factors, we will looks at the GoodmanKruskal tau values, which describe both the forward & backward association between 2 categorical variables. 
[GoodmanKruskal Tau][https://cran.r-project.org/web/packages/GoodmanKruskal/vignettes/GoodmanKruskal.html]

## GoodmanKruskal Method
Let's first consider the internal elements (categorical) of a house. 
In doing so, we will look at Utilities & Electrical, KitchenQual, BsmtQual, BsmtCond, BsmtExposure, HeatingQC, CentralAir, FireplaceQu

```{r}
require(GoodmanKruskal)

GKtau(train$Utilities, train$Electrical)

GKtau(train$BsmtQual, train$BsmtExposure)
GKtau(train$BsmtQual, train$BsmtCond)

GKtau(train$HeatingQC, train$Heating)
GKtau(train$HeatingQC, train$FireplaceQu)

GKtau(train$CentralAir, as.factor(train$SalePriceLog))

```
From the above we can see that knowledge of Utilities & Electical aren't quite useful in predicting eachother.
We can also see that both BsmtQual & BsmtExposure are predictive/ associated to eachother. However, we can see that knowledge of BsmtQual has a higher predictive association/ correlation to BsmtCond. Thus, if considering Bsmt features it seems BsmtQual is the best bet. 
Considering all Heating variables, it seems HeatingQC is the best one to consider as it has the highest predictive power, with the fewest number of levels. 

Now that we have investigated the internal elements, let's have a look at the external factors. 
These factors include: Roof
```{r}
GKtau(train$RoofStyle, train$RoofMatl) # RoofMatl higher predictive power
GKtau(train$RoofMatl, train$ExterQual)
GKtau(train$RoofMatl, train$ExterCond)

GKtau(train$OverallCond, train$OverallQual)
```


Knowledge of Neighborhood is predictive of MSZoning


```{r}

require(PCAmixdata)
# install.packages("GoodmanKruskal")

# Seperate categorical features
split = splitmix(train)
cat_train = split$X.quali

cat_train$SPLog = train$SalePriceLog
cat_train$SPLog

GKtau(train$Neighborhood, train$MSZoning)
table(train$Neighborhood, train$MSZoning)

GKtau(train$Neighborhood, train$MSZoning)
GKtau(train$Neighborhood, as.factor(train$SalePriceLog))

GKtau(train$Neighborhood, train$BldgType)
table(train$Neighborhood, train$BldgType)

GKtau(train$SaleCondition, train$SaleType)



GKtau(train$MSZoning, as.factor(train$SalePriceLog))
GKtau(as.factor(train$OverallQual), as.factor(train$SalePrice))

GK_matrix = GKtauDataframe(cat_train)
plot(GK_matrix, diagSize = 5)


```

## Insights from Categorical Graphs
From looking at the boxplots of the Categorical Varibles, it seems that the following variables show the most significant variance. 

These variables are:
- Neighborhood
- MSSubClass
- HouseStyle
- Exterior1st & Exterior2nd


 
 
 # Group Neighborhoods & MSZoning
 

-- Ensure Kendall is working as it needs
-- HouseStyle (Google definition)
-- Reduce vars into smaller groups 
-- Tree based (Categorical)
-- Clustering


## GoodmanKruskal Method
Knowledge of Neighborhood is predictive of MSZoning

```{r}
require(GoodmanKruskal)
require(PCAmixdata)
# install.packages("GoodmanKruskal")

# Seperate categorical features
split = splitmix(train)
cat_train = split$X.quali

cat_train$SPLog = train$SalePriceLog
cat_train$SPLog

GKtau(train$Neighborhood, train$MSZoning)
table(train$Neighborhood, train$MSZoning)

GKtau(train$Neighborhood, train$MSZoning)
GKtau(train$Neighborhood, as.factor(train$SalePriceLog))

GKtau(train$Neighborhood, train$BldgType)
table(train$Neighborhood, train$BldgType)

GKtau(train$SaleCondition, train$SaleType)



GKtau(train$MSZoning, as.factor(train$SalePriceLog))
GKtau(as.factor(train$OverallQual), as.factor(train$SalePrice))

GK_matrix = GKtauDataframe(cat_train)
plot(GK_matrix, diagSize = 5)


```


